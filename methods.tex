\documentclass{article}
\usepackage{xcolor}
\usepackage{lipsum}
\usepackage{amsmath}
\usepackage{empheq}
\usepackage{indentfirst}
\setlength{\parindent}{2em}
\usepackage{amssymb}
\usepackage[UTF8]{ctex}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{color}
\usepackage{tikz}
\author{My Name}
\title{The Title}
\begin{document}
	\section{abstract}
	distributed online learning,
	each learner optimizes its own learning parameter based on local data source and correspond with neighbours timely.
	
	privacy breaches(隐私泄露)
	
	centralized approach
	
	online data collection is inherently decentralized: data source are often widely distributed in different geographical locations.
	
	wide distribution:online learning
	high velocity:
	high dimensionality: sparse, spark and parallelize
	privacy concern: 
	
	exchange intermediate parameters with a random part of their own neighboring(spark trees: find a leaf and route to the corresponding sub-tree)
	
	distributed online learning
	
	similar to the mini-batch online learning. each iteration, the process receives K instances. Then each node processes one of the instances and updates its local model.
	These nodes communicate with each other to keep the consistency of their local model.
	
	A key factor in designing a distributed algorithm is the communication load between nodes.
	
	\section{INTRODUCTION}
	All nodes exchange their local parameter with their neighboring nodes.
	
	{\color{purple} For the privacy mechanism, differentially private framework is proposed with sensitive data}
	
	high-dimensional. 
	
	sparse solution.
	
%	\begin{tikzpicture}[sibling distance=10em,
%  every node/.style = {shape=rectangle, rounded corners,
%    draw, align=center,
%    top color=white, bottom color=blue!20}]]
%  \node {Sparse solution}
%    child { node {single-line} }
%    child { node {multi-line}
%      child { node {aligned at}
%        child { node {relation sign} }
%        child { node {several places} }
%        child { node {center} } }
%      child { node {first left,\\centered,\\last right} } };
%\end{tikzpicture}


\begin{tikzpicture}[sibling distance=20em,
  every node/.style = {shape=rectangle, rounded corners,
    draw, align=center,
    top color=white, bottom color=blue!20}]]
  \node {Sparse solution}
    child { node {truncated gradient} }
    child { node {dual averaging algorithm}
       } ;
\end{tikzpicture}

DOLA take online mirror descent and $Lasso-L_1$ norm.

\section{EXPERIMENTS}
	\textbf{step 1} regard one CPU as one node. and equally distribute the dataset to the m nodes.
	
	There are three different topologies.
	
	\begin{tikzpicture}[sibling distance=10em,
  every node/.style = {shape=rectangle, rounded corners,
    draw, align=center,
    top color=white, bottom color=blue!20}]]
  \node {topology}
    child { node {sparse} }
    child { node {more closely linked}}
    child { node {more intense}}
        ;
\end{tikzpicture}
	
	
	
	
	
	
	
	
	
\end{document}